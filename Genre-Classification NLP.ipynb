{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lyrics.csv', index_col='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock             131377\n",
       "Pop               49444\n",
       "Hip-Hop           33965\n",
       "Not Available     29814\n",
       "Metal             28408\n",
       "Other             23683\n",
       "Country           17286\n",
       "Jazz              17147\n",
       "Electronic        16205\n",
       "R&B                5935\n",
       "Indie              5732\n",
       "Folk               3241\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at genre value counts\n",
    "\n",
    "df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter DF to four genres\n",
    "genre_list = ['Rock', 'Pop', 'Country', 'Hip-Hop']\n",
    "\n",
    "\n",
    "df1 = df.loc[(df['genre'] == 'Rock') | (df['genre'] == 'Pop') | (df['genre'] == 'Country') | (df['genre'] == 'Hip-Hop')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232072, 5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the new dataframe\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "song          0\n",
       "year          0\n",
       "artist        0\n",
       "genre         0\n",
       "lyrics    43134\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#drop missing values from df\n",
    "df1.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188938, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"playin' everything so easy,\\nit's like you seem so sure.\\nstill your ways, you dont see\\ni'm not sure if they're for me.\\nthen things come right along our way, though we didn't truly ask.\\nit seems as if they're gonna linger\\nwith every delight they bring,\\njust like what you have truly seemed.\\ni'm trying to think of what you really want to say,\\neven through my darkest day.\\nyou might want to leave me,\\nfeeling strange about you\\nlike you're gonna let me know,\\nwhen words then slipped out of you.\\nwhen words dont come so easy to say\\nyou just leave me feeling, come what may\\nthough i want things coming from your way.\\ni say to you, you bore me all the time\\nwhen you seem to hold back all in you,\\nall that you want to let me know.\\nwhy dont you have the courage?\\nspeak up and i'll listen,\\nif you truly want me to know, then tell me.\\nis there something wrong with you\\nand you seem fastened there.\\nit sounds as if there'll be a melody\\nif things in you are let out\\nand then i will feel alright.\\nwhen you sleep, do you feel the same,\\nexactly as i do?\\ni really want to hear things from you,\\nthough i've felt something new\\neversince you acted that way.\\nif i go,\\nwould you still mind telling me?\\nif i stay,\\nyou seem to let the days go by.\\nif you truly want to let me know,\\nthen tell me.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song = df1.lyrics[1]\n",
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that clean and tokenizes lyrics\n",
    "\n",
    "def clean_tokenize_lyrics(song):\n",
    "    word_list = []\n",
    "    tokenized_lyrics = word_tokenize(song)\n",
    "     #remove all tokens that are not alphabetic\n",
    "    words = [word for word in tokenized_lyrics if word.isalpha()]\n",
    "    for word in words:\n",
    "        lower_word = word.lower()\n",
    "        word_list.append(lower_word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_tokenize_lyrics(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2afd11e6f8ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#clean lyrics for all songs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lyrics'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_tokenize_lyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "#clean lyrics for all songs\n",
    "df1['lyrics'] = df1.lyrics.apply(lambda x: clean_tokenize_lyrics(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemm Clean Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['cleaned_lyrics'] = df1['lyrics'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['oh', 'baby', 'how', 'you', 'doing', 'you', '...\n",
       "1         ['playin', 'everything', 'so', 'easy', 'it', '...\n",
       "2         ['if', 'you', 'search', 'for', 'tenderness', '...\n",
       "3         ['oh', 'oh', 'oh', 'i', 'oh', 'oh', 'oh', 'i',...\n",
       "4         ['party', 'the', 'people', 'the', 'people', 't...\n",
       "                                ...                        \n",
       "188933    ['i', 'got', 'ta', 'say', 'boy', 'after', 'onl...\n",
       "188934    ['i', 'helped', 'you', 'find', 'her', 'diamond...\n",
       "188935    ['look', 'at', 'the', 'couple', 'in', 'the', '...\n",
       "188936    ['when', 'i', 'fly', 'off', 'this', 'mortal', ...\n",
       "188937    ['i', 'heard', 'from', 'a', 'friend', 'of', 'a...\n",
       "Name: cleaned_lyrics, Length: 188938, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['cleaned_lyrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Clean Database as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv('clean_lyric_df.csv')\n",
    "df1 = pd.read_csv('clean_lyric_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to vectorize song\n",
    "\n",
    "def count_vectorize(song, vocab=None):\n",
    "    if vocab:\n",
    "        unique_words = vocab\n",
    "    else:\n",
    "        unique_words = list(set(song))\n",
    "    \n",
    "    song_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in song:\n",
    "        song_dict[word] += 1\n",
    "    \n",
    "    return song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW = count_vectorize(df1['lyrics'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create term frequency function\n",
    "def term_frequency(BoW_dict):\n",
    "    total_word_count = sum(BoW_dict.values())\n",
    "    \n",
    "    for ind, val in BoW_dict.items():\n",
    "        BoW_dict[ind] = val/ total_word_count\n",
    "    \n",
    "    return BoW_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of dictionaries\n",
    "def create_list_of_BoW(song_lyrics):\n",
    "\n",
    "    list_of_dictionaries = []\n",
    "\n",
    "    for song in song_lyrics:\n",
    "        BoW = count_vectorize(song)\n",
    "\n",
    "        list_of_dictionaries.append(BoW)\n",
    "    return list_of_dictionaries\n",
    "\n",
    "list_of_dictionaries = create_list_of_BoW(df1['lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(list_of_dicts):\n",
    "    vocab_set = set()\n",
    "    # Iterate through list of dfs and add index to vocab_set\n",
    "    for d in list_of_dicts:\n",
    "        for word in d.keys():\n",
    "            vocab_set.add(word)\n",
    "    \n",
    "    # Once vocab set is complete, create an empty dictionary with a key for each word and value of 0.\n",
    "    full_vocab_dict = {i:0 for i in vocab_set}\n",
    "    \n",
    "    # Loop through each word in full_vocab_dict\n",
    "    for word, val in full_vocab_dict.items():\n",
    "        docs = 0\n",
    "        \n",
    "        # Loop through list of dicts.  Each time a dictionary contains the word, increment docs by 1\n",
    "        for d in list_of_dicts:\n",
    "            if word in d:\n",
    "                docs += 1\n",
    "        \n",
    "        # Now that we know denominator for equation, compute and set IDF value for word\n",
    "        \n",
    "        full_vocab_dict[word] = np.log((len(list_of_dicts)/ float(docs)))\n",
    "    \n",
    "    return full_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_document_frequency(list_of_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.818px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
